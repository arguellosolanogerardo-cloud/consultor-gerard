import os
import json
import re
import colorama
import argparse
import threading
import itertools
import sys
import time
from dotenv import load_dotenv
import keyring
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from datetime import datetime

# Inicializamos colorama para que los colores funcionen en todas las terminales
colorama.init(autoreset=True)

# --- Carga la API Key ---
# Nota: no inicializamos la API ni recursos de red al importar el m√≥dulo.
# Creamos una funci√≥n para construir la cadena de recuperaci√≥n (llm + vectorstore)
def build_retrieval_chain(api_key: str):
    """Construye y devuelve el retrieval_chain usando la API key proporcionada.

    Carga el √≠ndice FAISS persistido en `faiss_index/`.
    """
    # Small helper to run blocking calls in a thread and show a spinner in console
    def run_with_spinner(func, *args, message="Procesando..."):
        result_holder = {}

        def target():
            try:
                result_holder['result'] = func(*args)
            except Exception as e:
                result_holder['error'] = e

        thread = threading.Thread(target=target)
        thread.start()

        spinner = itertools.cycle(['|', '/', '-', '\\'])
        sys.stdout.write(message + ' ')
        sys.stdout.flush()
        try:
            while thread.is_alive():
                sys.stdout.write(next(spinner))
                sys.stdout.flush()
                time.sleep(0.1)
                sys.stdout.write('\b')
        except KeyboardInterrupt:
            pass
        thread.join()

        sys.stdout.write('\r' + ' ' * (len(message) + 2) + '\r')

        if 'error' in result_holder:
            raise result_holder['error']
        return result_holder.get('result')

    # Load LLM and embeddings with spinner to give feedback for slow init
    llm = run_with_spinner(lambda: GoogleGenerativeAI(model="models/gemini-2.5-pro", google_api_key=api_key), message="Inicializando LLM...")
    embeddings = run_with_spinner(lambda: GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key), message="Inicializando embeddings...")

    try:
        vectorstore = run_with_spinner(lambda: FAISS.load_local(folder_path="faiss_index", embeddings=embeddings, allow_dangerous_deserialization=True), message="Cargando √≠ndice FAISS (puede tardar)...")
    except Exception as e:
        print(f"Error cargando FAISS index: {e}")
        raise

    retriever = vectorstore.as_retriever()
    retrieval_chain = (
        {
            "context": (lambda x: x["input"]) | retriever | format_docs_with_metadata,
            "input": (lambda x: x["input"]) 
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return retrieval_chain


def get_api_key():
    """Intentar obtener la API key de varias fuentes en orden:
    1. keyring del sistema (servicio 'consultor-gerard', nombre 'google_api_key')
    2. variable de entorno GOOGLE_API_KEY
    3. archivo .env (load_dotenv ya se usa en main)
    Devuelve la cadena o None si no se encuentra.
    """
    # 1) keyring
    try:
        kr = keyring.get_password('consultor-gerard', 'google_api_key')
        if kr:
            return kr
    except Exception:
        # Si keyring falla por cualquier motivo, lo ignoramos y continuamos
        pass

    # 2) environment
    api = os.environ.get('GOOGLE_API_KEY')
    if api:
        return api

    # 3) .env ya cargado por caller
    return None

# --- PERSONALIDAD DE "GERARD" (CON PROMPT CORREGIDO) ---
prompt = ChatPromptTemplate.from_template(r"""
üî¨ GERARD v3.01 - Sistema de An√°lisis Investigativo Avanzado
IDENTIDAD DEL SISTEMA
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Nombre: GERARD
Versi√≥n: 3.01 - Analista Forense Documental
Modelo Base: Gemini Pro Latest 2.5
Temperatura: 0.2-0.3 (M√°xima Precisi√≥n y Consistencia)
Especializaci√≥n: Criptoan√°lisis de Archivos .srt
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
MISI√ìN CR√çTICA
Eres GERARD, un sistema de inteligencia anal√≠tica especializado en arqueolog√≠a documental de archivos de subt√≠tulos (.srt). Tu prop√≥sito es descubrir patrones ocultos, mensajes encriptados y conexiones invisibles que emergen al correlacionar m√∫ltiples documentos mediante t√©cnicas forenses avanzadas.
Configuraci√≥n de Temperatura Optimizada (0.2-0.3)
Esta temperatura baja garantiza:
‚Ä¢ Consistencia absoluta entre consultas repetidas
‚Ä¢ Reproducibilidad de hallazgos para verificaci√≥n
‚Ä¢ Precisi√≥n quir√∫rgica en extracci√≥n de datos
‚Ä¢ Eliminaci√≥n de variabilidad en respuestas cr√≠ticas
‚Ä¢ Confiabilidad forense en an√°lisis investigativos
________________________________________
üö® PROTOCOLOS DE SEGURIDAD ANAL√çTICA
REGLAS ABSOLUTAS (Nivel de Cumplimiento: 100%)
üî¥ PROHIBICI√ìN NIVEL 1: FABRICACI√ìN DE DATOS
‚îú‚îÄ ‚ùå NO inventar informaci√≥n bajo ninguna circunstancia
‚îú‚îÄ ‚ùå NO usar conocimiento del modelo base (entrenamiento general)
‚îú‚îÄ ‚ùå NO suponer o inferir m√°s all√° de lo textualmente disponible
‚îî‚îÄ ‚ùå NO completar informaci√≥n faltante con l√≥gica externa

üî¥ PROHIBICI√ìN NIVEL 2: CONTAMINACI√ìN ANAL√çTICA
‚îú‚îÄ ‚ùå NO mezclar an√°lisis con citas textuales
‚îú‚îÄ ‚ùå NO parafrasear cuando se requiere texto literal
‚îú‚îÄ ‚ùå NO interpretar sin declarar expl√≠citamente que es interpretaci√≥n
‚îî‚îÄ ‚ùå NO omitir informaci√≥n contradictoria si existe

üü¢ MANDATOS OBLIGATORIOS
‚îú‚îÄ ‚úÖ Cada afirmaci√≥n DEBE tener cita textual verificable
‚îú‚îÄ ‚úÖ Cada cita DEBE incluir: [Documento] + [Timestamp] + [Texto Literal]
‚îú‚îÄ ‚úÖ Cada an√°lisis DEBE separarse claramente de evidencias
‚îú‚îÄ ‚úÖ Cada consulta DEBE ejecutar los 8 Protocolos de B√∫squeda Profunda
‚îî‚îÄ ‚úÖ Cada respuesta DEBE incluir nivel de confianza estad√≠stico
________________________________________
üîç SISTEMA DE AN√ÅLISIS MULTINIVEL
NIVEL 1: EXTRACCI√ìN SUPERFICIAL (Baseline)
Objetivo: Captura literal de informaci√≥n expl√≠cita
T√©cnica: Lectura directa y indexaci√≥n
Profundidad: 0-20% del contenido oculto
NIVEL 2: AN√ÅLISIS CORRELACIONAL (Intermediate)
Objetivo: Conexi√≥n de fragmentos dispersos
T√©cnicas:
    ‚îú‚îÄ Mapeo de relaciones tem√°ticas
    ‚îú‚îÄ Detecci√≥n de patrones recurrentes
    ‚îú‚îÄ Identificaci√≥n de complementariedades
    ‚îú‚îÄ Triangulaci√≥n de fuentes m√∫ltiples
    ‚îî‚îÄ Construcci√≥n de narrativas coherentes
Profundidad: 20-50% del contenido oculto
NIVEL 3: CRIPTOAN√ÅLISIS FORENSE (Advanced)
Objetivo: Descubrimiento de mensajes encriptados
Profundidad: 50-85% del contenido oculto
________________________________________
üîê PROTOCOLOS DE B√öSQUEDA PROFUNDA (8 CHECKS OBLIGATORIOS)
CHECK #1: AN√ÅLISIS ACR√ìSTICO MULTINIVEL
M√âTODO: ... (ejecutar los pasos descritos en el protocolo suministrado)
CHECK #2: AN√ÅLISIS DE PATRONES NUM√âRICOS
M√âTODO: ...
CHECK #3: AN√ÅLISIS DE PALABRAS CLAVE DISTRIBUIDAS
M√âTODO: ...
CHECK #4: AN√ÅLISIS SECUENCIAL CRONOL√ìGICO
M√âTODO: ...
CHECK #5: AN√ÅLISIS CONTEXTUAL DE FRAGMENTACI√ìN
M√âTODO: ...
CHECK #6: AN√ÅLISIS DE ANOMAL√çAS Y REPETICIONES
M√âTODO: ...
CHECK #7: AN√ÅLISIS DE OMISIONES DELIBERADAS
M√âTODO: ...
CHECK #8: AN√ÅLISIS DE METADATOS Y MARCADORES OCULTOS
M√âTODO: ...
________________________________________
üìã ESTRUCTURA DE RESPUESTA OPTIMIZADA PARA TEMP 0.2-0.3
FORMATO ESTANDARIZADO (Reproducibilidad Garantizada)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üî¨ AN√ÅLISIS
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Timestamp de An√°lisis: [{date}]
Consulta Procesada: "{input}"
Temperatura Operativa: 0.2-0.3
Hash de Sesi√≥n: [{session_hash}]
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
SECCI√ìN 1: S√çNTESIS INVESTIGATIVA
[Resuma hallazgos y evidencias; siga estrictamente las reglas de cita y separaci√≥n de evidencia/interpretaci√≥n]
SECCI√ìN 2: EVIDENCIA FORENSE ESTRUCTURADA
[Agrupe por documento y cite por timestamp: siempre texto literal]
SECCI√ìN 3: √çNDICE DE FUENTES Y MAPEO
[Reporte de cobertura y relevancia]
SECCI√ìN 4: METADATOS Y GARANT√çA DE CALIDAD
[Reporte de ejecuci√≥n de checks y nivel de confianza]
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
FIN DEL AN√ÅLISIS

Bas√°ndote estrictamente en el contenido disponible en el contexto (no accedas a fuentes externas), responde la consulta del usuario respetando todas las prohibiciones y mandatos arriba definidos.
""")

# --- FUNCI√ìN PARA FORMATEAR DOCUMENTOS (CON LIMPIEZA REFORZADA) ---
def get_cleaning_pattern():
    """Crea un patr√≥n de regex robusto para eliminar textos no deseados."""
    texts_to_remove = [
        '[Spanish (auto-generated)]',
        '[DownSub.com]',
        '[M√∫sica]',
        '[Aplausos]'
    ]
    # Este patr√≥n es m√°s robusto: busca el texto dentro de los corchetes,
    # permitiendo espacios en blanco opcionales alrededor.
    robust_patterns = [r'\[\s*' + re.escape(text[1:-1]) + r'\s*\]' for text in texts_to_remove]
    return re.compile(r'|'.join(robust_patterns), re.IGNORECASE)

cleaning_pattern = get_cleaning_pattern()

def format_docs_with_metadata(docs):
    """Prepara los documentos recuperados, limpiando robustamente el contenido y los timestamps."""
    formatted_strings = []
    for doc in docs:
        source_filename = os.path.basename(doc.metadata.get('source', 'Fuente desconocida'))
        
        # 1. Limpieza de textos no deseados
        cleaned_content = cleaning_pattern.sub('', doc.page_content)

        # 2. ¬°NUEVO! Eliminar milisegundos de los timestamps
        # El patr√≥n busca HH:MM:SS,ms y lo reemplaza con HH:MM:SS
        cleaned_content = re.sub(r'(\d{2}:\d{2}:\d{2}),\d{3}', r'\1', cleaned_content)
        
        # 3. Limpieza de l√≠neas vac√≠as
        cleaned_content = "\n".join(line for line in cleaned_content.split('\n') if line.strip())
        
        if cleaned_content:
            formatted_strings.append(f"Fuente del Archivo: {source_filename}\nContenido:\n{cleaned_content}")
            
    return "\n\n---\n\n".join(formatted_strings)

# --- Cadena de recuperaci√≥n (LCEL) ---
# ... el retrieval_chain se construye con `build_retrieval_chain(api_key)` cuando
# se ejecute el script como programa principal.

# --- NUEVA FUNCI√ìN para convertir el JSON a texto plano para el log ---
def get_clean_text_from_json(json_string):
    """Convierte la respuesta JSON en una cadena de texto simple y legible."""
    try:
        match = re.search(r'\[.*\]', json_string, re.DOTALL)
        if not match:
            return json_string

        data = json.loads(match.group(0))
        full_text = "".join([item.get("content", "") for item in data])
        return full_text
    except Exception:
        return json_string

# --- NUEVA FUNCI√ìN para guardar la conversaci√≥n en un archivo ---
def save_to_log(question, user, answer_json):
    """Guarda la pregunta y la respuesta en un archivo de registro."""
    clean_answer = get_clean_text_from_json(answer_json)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:M:%S")
    
    with open("gerard_log.txt", "a", encoding="utf-8") as f:
        f.write(f"--- Conversaci√≥n del {timestamp} ---\n")
        f.write(f"Usuario: {user}\n")
        f.write(f"Pregunta: {question}\n")
        f.write(f"Respuesta de GERARD: {clean_answer}\n")
        f.write("="*40 + "\n\n")

# --- FUNCI√ìN PARA IMPRIMIR LA RESPUESTA CON M√öLTIPLES COLORES ---
def print_json_answer(json_string):
    # Failsafe: volvemos a limpiar la respuesta final por si acaso
    cleaned_string = cleaning_pattern.sub('', json_string)
    
    try:
        match = re.search(r'\[.*\]', cleaned_string, re.DOTALL)
        if not match:
            print(f"{colorama.Fore.RED}Respuesta no es un JSON v√°lido:\n{cleaned_string}")
            return

        data = json.loads(match.group(0))
        
        for item in data:
            content_type = item.get("type", "normal")
            content = item.get("content", "")
            
            if content_type == "emphasis":
                print(f"{colorama.Fore.YELLOW}{content}", end="")
            else:
                parts = re.split(r'(\(.*?\))', content)
                for part in parts:
                    if part.startswith('(') and part.endswith(')'):
                        print(f"{colorama.Fore.MAGENTA}{part}", end="")
                    else:
                        print(f"{colorama.Style.RESET_ALL}{part}", end="")
        print()
    except json.JSONDecodeError:
        print(f"{colorama.Fore.RED}Error: El modelo no devolvi√≥ un JSON v√°lido. Respuesta recibida:\n{cleaned_string}")
    except Exception as e:
        print(f"{colorama.Fore.RED}Ocurri√≥ un error inesperado al procesar la respuesta: {e}")

# --- Bucle de Interacci√≥n ---
def main():
    """Funci√≥n principal que lanza el loop interactivo. Protegida para que no se ejecute al importar."""
    parser = argparse.ArgumentParser()
    parser.add_argument("--fast", action="store_true", help="Modo r√°pido: evita cargar FAISS/LLM y usa una respuesta simulada")
    args = parser.parse_args()

    load_dotenv()

    # Fast mode: no inicializar LLM/FAISS (√∫til para pruebas r√°pidas)
    if args.fast:
        class DummyChain:
            def invoke(self, payload):
                # Respuesta de ejemplo m√°s fiel al formato GERARD v3.01
                q = payload.get('input', '')
                ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                session_hash = f"demo-{ts.replace(' ', 'T')}"
                # Ejemplo de fuente y timestamp que imita la estructura real
                fuente = "ejemplo.srt"
                timestamp_span = "00:01:23 --> 00:01:26"

                response = [
                    {
                        "type": "normal",
                        "content": f"üî¨ AN√ÅLISIS\nTimestamp de An√°lisis: [{ts}]\nConsulta Procesada: \"{q}\"\nSECCI√ìN 1: S√çNTESIS INVESTIGATIVA\nResumen: En modo demo se simula la detecci√≥n de coincidencias textuales entre subt√≠tulos."
                    },
                    {
                        "type": "emphasis",
                        "content": f"(Fuente: {fuente}, Timestamp: {timestamp_span})"
                    },
                    {
                        "type": "normal",
                        "content": f"SECCI√ìN 2: EVIDENCIA FORENSE ESTRUCTURADA\n- Texto literal: \"simulaci√≥n de texto coincidente\" (Fuente: {fuente}, Timestamp: {timestamp_span})\nSECCI√ìN 4: METADATOS Y GARANT√çA DE CALIDAD\n- Checks ejecutados: [CHECK #1: OK, CHECK #2: OK]\nNivel de confianza: 0.65\nHash de Sesi√≥n: [{session_hash}]"
                    }
                ]

                return json.dumps(response, ensure_ascii=False)

        retrieval_chain = DummyChain()
    else:
        # Intentar obtener la key desde keyring o entornos
        api_key = get_api_key()
        if not api_key:
            print("Error: No se encontr√≥ la API key. Guarda la clave en el keyring (scripts/store_key_keyring.py), en la variable de entorno GOOGLE_API_KEY, o en .env")
            return

        retrieval_chain = build_retrieval_chain(api_key)

    print("GERARD listo. Escribe tu pregunta o 'salir' para terminar.")
    user_name = input("Por favor, introduce tu nombre para comenzar: ")

    while True:
        prompt_text = f"\nTu pregunta {colorama.Fore.BLUE}{user_name.upper()}{colorama.Style.RESET_ALL}: "
        pregunta = input(prompt_text)

        if pregunta.lower() == 'salir':
            break

        print("Buscando...")
        try:
            answer = retrieval_chain.invoke({"input": pregunta})
            print("\nRespuesta de GERARD:")
            print_json_answer(answer)
            save_to_log(pregunta, user_name.upper(), answer)

        except Exception as e:
            print(f"\n{colorama.Fore.RED}Ocurri√≥ un error al procesar tu pregunta: {e}")


if __name__ == "__main__":
    main()

