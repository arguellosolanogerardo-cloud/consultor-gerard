#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SCRIPT DE RE-INDEXACIÃ“N CON CHUNKS PEQUEÃ‘OS
============================================
Este script re-crea el Ã­ndice FAISS con chunks MÃS PEQUEÃ‘OS
para mejorar la precisiÃ³n de bÃºsqueda.

MEJORAS:
âœ“ chunk_size: 1000 â†’ 500 (chunks mÃ¡s pequeÃ±os)
âœ“ chunk_overlap: 200 â†’ 100
âœ“ Mejor recall en bÃºsquedas especÃ­ficas
âœ“ Menos diluciÃ³n semÃ¡ntica

USO SIMPLE:
    python reiniciar_indice.py

AUTOMÃTICO:
- Hace backup del Ã­ndice anterior
- Procesa todos los .srt
- Crea nuevo Ã­ndice optimizado
- Verifica con bÃºsqueda de prueba
"""

import os
import sys
import shutil
from datetime import datetime
from dotenv import load_dotenv

# Cargar API key
load_dotenv()

if not os.getenv('GOOGLE_API_KEY'):
    print("âŒ ERROR: Falta GOOGLE_API_KEY")
    print("\nConfigure con:")
    print("  $env:GOOGLE_API_KEY = 'tu-api-key'")
    sys.exit(1)

print("ðŸ”§ Importando librerÃ­as...")
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS

# === CONFIGURACIÃ“N ===
DOCS_DIR = "documentos_srt"
FAISS_DIR = "faiss_index"
BACKUP_DIR = f"faiss_index_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

# NUEVOS PARÃMETROS (chunks mÃ¡s pequeÃ±os)
CHUNK_SIZE = 500      # Antes: 1000
CHUNK_OVERLAP = 100   # Antes: 200

print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        RE-INDEXACIÃ“N OPTIMIZADA - CHUNKS PEQUEÃ‘OS        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“¦ Chunk size: {CHUNK_SIZE} (antes: 1000) - 50% mÃ¡s pequeÃ±o
ðŸ”— Overlap: {CHUNK_OVERLAP} (antes: 200)
ðŸ“‚ Directorio: {DOCS_DIR}
ðŸŽ¯ Ãndice: {FAISS_DIR}
""")

# === 1. BACKUP ===
print("\n" + "="*60)
print("1ï¸âƒ£  BACKUP DEL ÃNDICE ANTERIOR")
print("="*60)

if os.path.exists(FAISS_DIR):
    try:
        shutil.copytree(FAISS_DIR, BACKUP_DIR)
        print(f"âœ… Backup: {BACKUP_DIR}")
        shutil.rmtree(FAISS_DIR)
        print(f"âœ… Ãndice anterior eliminado")
    except Exception as e:
        print(f"âš ï¸ Error en backup: {e}")
else:
    print("â„¹ï¸ No hay Ã­ndice anterior")

# === 2. CARGAR DOCUMENTOS ===
print("\n" + "="*60)
print("2ï¸âƒ£  CARGANDO ARCHIVOS .SRT")
print("="*60)

try:
    loader = DirectoryLoader(
        DOCS_DIR,
        glob="**/*.srt",
        loader_cls=TextLoader,
        loader_kwargs={'encoding': 'utf-8'},
        show_progress=True
    )
    documents = loader.load()
    print(f"âœ… {len(documents)} archivos cargados")
    
    total_chars = sum(len(doc.page_content) for doc in documents)
    print(f"   {total_chars:,} caracteres totales")
    
except Exception as e:
    print(f"âŒ ERROR: {e}")
    sys.exit(1)

# === 3. DIVIDIR EN CHUNKS ===
print("\n" + "="*60)
print("3ï¸âƒ£  DIVIDIENDO EN CHUNKS PEQUEÃ‘OS")
print("="*60)

try:
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    
    print(f"âœ… {len(chunks)} chunks creados")
    print(f"   {len(chunks) // len(documents)} chunks por documento (promedio)")
    
    sizes = [len(c.page_content) for c in chunks]
    print(f"   TamaÃ±o promedio: {sum(sizes)//len(sizes)} caracteres")
    print(f"   Rango: {min(sizes)} - {max(sizes)} caracteres")
    
except Exception as e:
    print(f"âŒ ERROR: {e}")
    sys.exit(1)

# === 4. CREAR EMBEDDINGS ===
print("\n" + "="*60)
print("4ï¸âƒ£  INICIALIZANDO EMBEDDINGS")
print("="*60)

try:
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    print("âœ… Embeddings de Google listos")
except Exception as e:
    print(f"âŒ ERROR: {e}")
    sys.exit(1)

# === 5. CREAR ÃNDICE FAISS ===
print("\n" + "="*60)
print("5ï¸âƒ£  CREANDO ÃNDICE FAISS")
print("="*60)
print("â³ Procesando en batches (puede tomar varios minutos)...\n")

try:
    BATCH_SIZE = 100
    vectorstore = None
    total_batches = (len(chunks) - 1) // BATCH_SIZE + 1
    
    for i in range(0, len(chunks), BATCH_SIZE):
        batch = chunks[i:i+BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        print(f"   Batch {batch_num}/{total_batches} ({len(batch)} chunks)...")
        
        if vectorstore is None:
            vectorstore = FAISS.from_documents(batch, embeddings)
        else:
            batch_vs = FAISS.from_documents(batch, embeddings)
            vectorstore.merge_from(batch_vs)
    
    print(f"\nâœ… Ãndice FAISS creado: {len(chunks)} chunks")
    
except Exception as e:
    print(f"âŒ ERROR: {e}")
    sys.exit(1)

# === 6. GUARDAR ===
print("\n" + "="*60)
print("6ï¸âƒ£  GUARDANDO ÃNDICE")
print("="*60)

try:
    vectorstore.save_local(FAISS_DIR)
    print(f"âœ… Ãndice guardado: {FAISS_DIR}")
    
    size_mb = sum(
        os.path.getsize(os.path.join(FAISS_DIR, f))
        for f in os.listdir(FAISS_DIR)
    ) / 1024 / 1024
    
    print(f"   TamaÃ±o: {size_mb:.2f} MB")
    
except Exception as e:
    print(f"âŒ ERROR: {e}")
    sys.exit(1)

# === 7. VERIFICAR ===
print("\n" + "="*60)
print("7ï¸âƒ£  VERIFICACIÃ“N")
print("="*60)

try:
    test_vs = FAISS.load_local(FAISS_DIR, embeddings, allow_dangerous_deserialization=True)
    print(f"âœ… Ãndice verificado: {test_vs.index.ntotal} documentos")
    
    # BÃºsqueda de prueba
    print("\nðŸ§ª PRUEBA DE BÃšSQUEDA:")
    test_query = "linaje ra tric jac bis"
    results = test_vs.similarity_search_with_score(test_query, k=5)
    
    print(f"   Query: '{test_query}'")
    print(f"   Resultados: {len(results)}")
    
    if results:
        doc, score = results[0]
        source = doc.metadata.get('source', 'desconocido')
        filename = source.split('\\')[-1] if '\\' in source else source
        
        print(f"\n   Top resultado:")
        print(f"   â€¢ Score: {score:.4f}")
        print(f"   â€¢ Fuente: {filename[:60]}")
        print(f"   â€¢ Preview: {doc.page_content[:150]}...")
        
        # Verificar si encuentra el documento correcto
        if "DESCUBRIENDO" in source:
            print("\n   âœ… Â¡Encuentra el documento correcto!")
        else:
            print("\n   âš ï¸ Top resultado no es el esperado")
            print("      (Pero con k=50 deberÃ­a estar en los resultados)")
    
except Exception as e:
    print(f"âŒ ERROR: {e}")
    sys.exit(1)

# === RESUMEN ===
print("\n" + "="*60)
print("âœ… RE-INDEXACIÃ“N COMPLETADA")
print("="*60)

print(f"""
ðŸ“Š ESTADÃSTICAS:
   â€¢ Archivos: {len(documents)}
   â€¢ Chunks: {len(chunks)} (antes: ~{len(chunks)//2})
   â€¢ Chunk size: {CHUNK_SIZE} caracteres (antes: 1000)
   â€¢ TamaÃ±o Ã­ndice: {size_mb:.2f} MB
   â€¢ Backup: {BACKUP_DIR}

ðŸŽ¯ MEJORAS:
   âœ“ Chunks 50% mÃ¡s pequeÃ±os
   âœ“ Mayor precisiÃ³n en bÃºsquedas
   âœ“ Menos diluciÃ³n semÃ¡ntica
   âœ“ k=50 en consultar_web.py

ðŸš€ PRÃ“XIMO PASO:
   Reinicia Streamlit:
   
   > Get-Process | Where-Object {{$_.ProcessName -eq "streamlit"}} | Stop-Process -Force
   > streamlit run consultar_web.py
   
ðŸ’¡ Para agregar mÃ¡s documentos en el futuro:
   1. Copia nuevos .srt a {DOCS_DIR}/
   2. Ejecuta: python reiniciar_indice.py
   3. Reinicia Streamlit
""")

print("="*60)
print("ðŸŽ‰ Â¡LISTO PARA USAR!")
print("="*60)
